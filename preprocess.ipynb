{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aboriginal-vitamin",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/training.1600000.processed.noemoticon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Read Data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/training.1600000.processed.noemoticon.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlatin\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39m# Data reduction\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/training.1600000.processed.noemoticon.csv'"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('data/training.1600000.processed.noemoticon.csv', header=None, encoding='latin')\n",
    "df.columns = ['label', 'id', 'date', 'query', 'user', 'tweet']\n",
    "\n",
    "# Data reduction\n",
    "df = df.drop(['id', 'date', 'query', 'user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {0:'Negative', 2:'Neutral', 4:'Positive'}\n",
    "\n",
    "def convert_labels(label):\n",
    "    return labels_dict[label]\n",
    "\n",
    "df.label = df.label.apply(lambda x: convert_labels(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = df.label.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(instances.index, instances.values)\n",
    "plt.title(\"Data Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-diagnosis",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "punctuations_and_dummies = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def preprocess(df, will_be_stemmed=False):\n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row.tweet\n",
    "        tweet = re.sub(punctuations_and_dummies, ' ', str(tweet).lower()).strip()\n",
    "        tokens = []\n",
    "        for token in tweet.split():\n",
    "            if token not in stop_words:\n",
    "                if will_be_stemmed:\n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "        df.tweet = \" \".join(tokens)\n",
    "\n",
    "\n",
    "preprocess(df.tweet)\n",
    "'''\n",
    "\n",
    "\n",
    "def preprocess(tweet, will_be_stemmed=False):\n",
    "        tweet = re.sub(punctuations_and_dummies, ' ', str(tweet).lower()).strip()\n",
    "        tokens = []\n",
    "        for token in tweet.split():\n",
    "            if token not in stop_words:\n",
    "                if will_be_stemmed:\n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "df.tweet = df.tweet.apply(lambda tw: preprocess(tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 0 length tweets\n",
    "df = df[df.iloc[:,1].astype(str).str.len()!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_len = [len(x) for x in df['tweet']]\n",
    "pd.Series(tweets_len).hist()\n",
    "plt.show()\n",
    "pd.Series(tweets_len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-demographic",
   "metadata": {},
   "source": [
    "### Number of Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_str = \"\"\n",
    "for i in df.tweet:\n",
    "    all_str += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "letter_list = list(all_str)\n",
    "my_counter = Counter(letter_list)\n",
    "\n",
    "letter_df = pd.DataFrame.from_dict(my_counter, orient='index').reset_index()\n",
    "letter_df = letter_df.rename(columns={'index':'letter', 0:'frequency'})\n",
    "letter_df = letter_df.loc[letter_df['letter'].isin(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])]\n",
    "letter_df['all_tweets_relative_freq']=letter_df['frequency']/letter_df['frequency'].sum()\n",
    "letter_df = letter_df.sort_values('letter')\n",
    "\n",
    "english = pd.read_csv('data/letter_frequency_en_US.csv')\n",
    "english['expected_relative_frequency'] = english['count']/english['count'].sum()\n",
    "english = english.drop(['count'], axis=1)\n",
    "\n",
    "letter_df = pd.merge(letter_df, english, on='letter')\n",
    "letter_df['expected'] = np.round(letter_df['expected_relative_frequency']*letter_df['frequency'].sum(),0)\n",
    "letter_df = letter_df.reset_index().drop(['index'], axis=1)\n",
    "letter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_df.plot(x=\"letter\", y=[\"all_tweets_relative_freq\", \"expected_relative_frequency\"], kind=\"barh\", figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-philadelphia",
   "metadata": {},
   "source": [
    "#### Compare the Observed Frequencies with the Expected Frequencies in English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "# Chi-square test of independence.\n",
    "c, p, dof, expected = chi2_contingency(letter_df[['frequency', 'expected']])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-theta",
   "metadata": {},
   "source": [
    "We get that the p-value (p) is 0 which implies that the letter frequency does not follow the same distribution with what we see in English tests, although the Pearson correlation is too high (~96.7%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-discretion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "letter_df[['frequency', 'expected']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "df1['number_of_characters'] = [len(tw) for tw in df1.tweet]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-inspector",
   "metadata": {},
   "source": [
    "## Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['number_of_words'] = [len(tw.split()) for tw in df1.tweet]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-matthew",
   "metadata": {},
   "source": [
    "### Positives + Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-gentleman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from wordcloud import WordCloud\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "all_tweets = ' '.join(df['tweet'].str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "    \n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in whole tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-parks",
   "metadata": {},
   "source": [
    "### Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-peninsula",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(df[df.label == 'Positive'].tweet.str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "    \n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in positive tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 25))\n",
    "plt.axis('off')\n",
    "wordcloud_fig = WordCloud(max_words = 2000 , width = 1600 , height = 800, background_color ='white', min_font_size = 10).generate(\" \".join(df[df.label == 'Positive'].tweet))\n",
    "plt.imshow(wordcloud_fig, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-discipline",
   "metadata": {},
   "source": [
    "### Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(df[df.label == 'Negative'].tweet.str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "    \n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in negative tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize = (25, 25))\n",
    "plt.axis('off')\n",
    "wordcloud_fig = WordCloud(max_words = 2000 , width = 1600 , height = 800, background_color ='white', min_font_size = 10).generate(\" \".join(df[df.label == 'Negative'].tweet))\n",
    "plt.imshow(wordcloud_fig, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-bundle",
   "metadata": {},
   "source": [
    "### Training Data and Test Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=7)\n",
    "print('Training Data', len(train_data), 'Test Data', len(test_data))\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-success",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data.tweet)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-improvement",
   "metadata": {},
   "source": [
    "### GLOVE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = 'models'\n",
    "EMBEDDING_DIMENSION = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "glove_file = open('glove/glove.6B.300d.txt', encoding='utf8')\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = value = values[0]\n",
    "    coefficients = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefficients\n",
    "glove_file.close()\n",
    "\n",
    "print('%s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIMENSION))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIMENSION, weights=[embedding_matrix], input_length=30, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-emergency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-campus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
